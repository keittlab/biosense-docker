#!/usr/bin/env python3

import os
import sys
import time
import signal
import shutil
import re
from datetime import datetime, timedelta
import logging
from logging.handlers import RotatingFileHandler

from sqlalchemy import (
    create_engine,
    Column,
    Integer,
    String,
    Float,
    DateTime,
    Index,
    Boolean,
    inspect,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

import toml
from birdnetlib import Recording
from birdnetlib.analyzer import Analyzer


# Configure logging
log_dir = "/var/log/biosense/"
log_file = os.path.join(log_dir, "run-birdnet.log")
log_handler = RotatingFileHandler(
    log_file, maxBytes=1 * 1024 * 1024, backupCount=5
)  # 1 MB per file, 5 backup files
logging.basicConfig(
    level=logging.INFO,
    handlers=[log_handler],
    format="%(asctime)s - %(levelname)s - %(message)s",
)


# Function to read configuration with error handling
def read_config(config_file):
    try:
        if not os.path.exists(config_file):
            raise Exception(f"Configuration file does not exist: {config_file}")

        logging.info(f"Reading configuration file: {config_file}")
        with open(config_file, "r") as f:
            content = toml.load(f)

        if not content:
            raise Exception("Configuration file is empty or not found.")

        return content["DEFAULT"]
    except Exception as e:
        logging.error(f"Error reading configuration file: {e}")
        sys.exit(1)


# Read configuration
config_file = "/etc/biosense/run-birdnet.conf"
config = read_config(config_file)

RECORDING_DIR = config.get(
    "RECORDING_DIR", "/home/biosense/datastore/sound_recordings/"
)
EXPORT_DIR = config.get(
    "EXPORT_DIR", "/home/biosense/datastore/sound_analysis_results/"
)
PROCESSED_DIR = config.get(
    "PROCESSED_DIR", "/home/biosense/datastore/processed_recordings/"
)
SEGMENT_LENGTH = int(config.get("SEGMENT_LENGTH", 3))
DB_PATH = config.get(
    "DB_PATH", "/home/biosense/datastore/birdnet_results.db"
)  # For SQLite
MIN_CONF = float(config.get("MIN_CONF", 0.1))  # Read min_conf from config
OVERLAP = float(config.get("OVERLAP", 1.5))  # Read overlap from config
SENSITIVITY = float(config.get("SENSITIVITY", 1.0))  # Read sensitivity from config

# Handle empty or invalid latitude/longitude values
try:
    LATITUDE = float(os.getenv("LATITUDE", "0.0"))
    LONGITUDE = float(os.getenv("LONGITUDE", "0.0"))
except ValueError:
    LATITUDE, LONGITUDE = 0.0, 0.0
    logging.warning(
        "Invalid LATITUDE or LONGITUDE environment variable; defaulting to 0.0 for both."
    )

HOSTNAME = os.uname().nodename.replace("-", "_")

# SQLAlchemy setup
Base = declarative_base()


class Detection(Base):
    __tablename__ = "birdnet_detections"
    id = Column(Integer, primary_key=True, autoincrement=True)
    hostname = Column(String)
    latitude = Column(Float)
    longitude = Column(Float)
    timestamp = Column(DateTime)  # Timestamp of the recording
    start_time = Column(Float)
    end_time = Column(Float)
    sci_name = Column(String, nullable=False)
    com_name = Column(String, nullable=False)
    confidence = Column(Float)
    is_likely = Column(Boolean, nullable=True)
    source_recording = Column(String, nullable=True)
    file_path = Column(String, nullable=True)

    __table_args__ = (
        Index(
            "idx_com_name_timestamp", "com_name", "timestamp"
        ),  # Filter by species and date
        Index(
            "idx_start_end_time", "start_time", "end_time"
        ),  # Range-based queries for overlapping records
        Index(
            "idx_com_name_confidence", "com_name", "confidence"
        ),  # Quickly find lowest confidence by species
        Index("idx_sci_name", "sci_name"),  # Index for scientific name
    )


# Create the engine
engine = create_engine(f"sqlite:///{DB_PATH}")

# Ensure that the tables and indices are created
Base.metadata.create_all(engine)


# Define session creation function
def create_session(engine):
    Session = sessionmaker(bind=engine)
    return Session()


# Initialize the session
session = create_session(engine)


def validate_database(engine, expected_columns):
    try:
        inspector = inspect(engine)
        if "birdnet_detections" in inspector.get_table_names():
            columns = inspector.get_columns("birdnet_detections")
            column_names = [column["name"] for column in columns]
            if column_names == expected_columns:
                logging.info("Database schema is valid.")
                return True
            else:
                logging.warning(
                    f"Database schema mismatch. Expected columns: {expected_columns}, Found columns: {column_names}"
                )
        else:
            logging.warning("Table 'birdnet_detections' does not exist.")
        return False
    except Exception as e:
        logging.error(f"Error inspecting database schema: {e}")
        return False


def initialize_database(db_path, expected_columns):
    if os.path.exists(db_path) and not validate_database(engine, expected_columns):
        backup_db_path = f"{db_path}.backup_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        logging.warning(
            f"Database schema does not match. Moving existing database to {backup_db_path}"
        )
        shutil.move(db_path, backup_db_path)
        Base.metadata.create_all(engine)
    return create_session(engine)


# Expected columns in the database, maintaining order
expected_columns = [
    "id",
    "hostname",
    "latitude",
    "longitude",
    "timestamp",
    "start_time",
    "end_time",
    "sci_name",
    "com_name",
    "confidence",
    "is_likely",
    "source_recording",
    "file_path",
]

# Initialize database
session = initialize_database(DB_PATH, expected_columns)


def get_creation_time(filepath):
    try:
        # Extract initial timestamp and segment number from the filename
        match = re.search(
            r"_(\d{8}_\d{6})-(\d+)", filepath
        )  # Pattern to match "_YYYYMMDD_HHMMSS-segment"
        if match:
            timestamp_str = match.group(1)
            segment_number = int(match.group(2))

            # Parse the initial timestamp
            initial_time = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")

            # Calculate the exact start time for this segment
            segment_offset = timedelta(minutes=15 * (segment_number - 1))
            return initial_time + segment_offset
        else:
            logging.error(
                f"No valid timestamp or segment number found in filename: {filepath}"
            )
            return None
    except Exception as e:
        logging.error(f"Error parsing timestamp from filename {filepath}: {e}")
        return None


def create_partitioned_dir(base_dir, timestamp):
    year = timestamp.strftime("%Y")
    month = timestamp.strftime("%m")
    day = timestamp.strftime("%d")
    partitioned_dir = os.path.join(base_dir, year, month, day)
    os.makedirs(partitioned_dir, exist_ok=True)
    return partitioned_dir


def on_analyze_complete(recording, recording_timestamp):
    logging.info(f"Analyzed: {recording.path}")
    logging.debug(f"Detections: {recording.detections}")

    # Check for any human vocal detections
    if any(
        detection.get("common_name") == "Human vocal"
        for detection in recording.detections
    ):
        try:
            os.remove(recording.path)
            logging.info(
                f"Deleted source recording file due to human vocal detection: {recording.path}"
            )
            return None  # Return None to indicate the file was deleted
        except Exception as e:
            logging.error(f"Error deleting source recording file {recording.path}: {e}")
            return recording.path  # Return the original path if deletion failed

    # Move the recording to the processed directory after successful analysis
    try:
        processed_dir_partitioned = create_partitioned_dir(
            PROCESSED_DIR, recording_timestamp
        )
        destination = os.path.join(
            processed_dir_partitioned, os.path.basename(recording.path)
        )
        shutil.move(recording.path, destination)
        logging.debug(f"Moved recording to processed directory: {destination}")
        return destination
    except Exception as e:
        logging.error(f"Error moving recording: {recording.path} - {e}")
        return recording.path


def on_error(recording, error):
    logging.error(f"Error: {error} in {recording.path}")
    # Attempt to delete the corrupted .flac file
    try:
        os.remove(recording.path)
        logging.info(f"Deleted corrupt .flac file: {recording.path}")
    except Exception as e:
        logging.error(f"Failed to delete corrupt .flac file {recording.path}: {e}")


def extract_detected_segments(recording, recording_timestamp, source_recording_path):
    export_dir_partitioned = create_partitioned_dir(EXPORT_DIR, recording_timestamp)
    if not os.path.exists(export_dir_partitioned):
        os.makedirs(export_dir_partitioned)

    # Extract audio segments of detections using BirdNET's method
    recording.extract_detections_as_audio(directory=export_dir_partitioned)

    # Process each detection segment
    for detection in recording.detections:
        old_segment_filename = detection["extracted_audio_path"]

        # Determine the new segment filename path
        new_segment_filename = os.path.join(
            export_dir_partitioned, os.path.basename(old_segment_filename)
        )

        if not old_segment_filename or not os.path.exists(old_segment_filename):
            logging.error(
                f"Extracted file not found or is None: {old_segment_filename}"
            )
            new_segment_filename = None  # Set to None to log as NULL in SQLite
        else:
            # Check if the detected sound is human vocal
            if detection.get("common_name") == "Human vocal":
                # If human voice is detected, delete the segment file and set filename to None
                try:
                    os.remove(old_segment_filename)
                    logging.info(
                        f"Deleted human vocal segment file: {old_segment_filename}"
                    )
                    new_segment_filename = None  # Set to None to log as NULL in SQLite
                except Exception as e:
                    logging.error(
                        f"Error deleting human vocal file {old_segment_filename}: {e}"
                    )
            else:
                # Move the non-human voice segment to the new location
                shutil.move(old_segment_filename, new_segment_filename)
                logging.debug(f"Extracted segment: {new_segment_filename}")

        # Save detection to the database
        detection_data = Detection(
            hostname=HOSTNAME,
            latitude=LATITUDE,
            longitude=LONGITUDE,
            timestamp=recording_timestamp,
            start_time=detection["start_time"],
            end_time=detection["end_time"],
            sci_name=detection["scientific_name"],
            com_name=detection["common_name"],
            confidence=detection["confidence"],
            is_likely=detection["is_predicted_for_location_and_date"],
            source_recording=source_recording_path,
            file_path=new_segment_filename,  # This will be NULL in the database if None
        )
        try:
            logging.debug(
                f"Attempting to write detection to database: {detection_data}"
            )
            session.add(detection_data)
            session.commit()
            logging.debug("Detection written to database successfully.")
        except Exception as e:
            logging.error(f"Error writing to database: {e}")
            session.rollback()


def analyze_recordings():
    try:
        analyzer = Analyzer()
    except Exception as e:
        logging.error(f"Failed to initialize Analyzer: {e}")
        return

    while True:
        logging.info("Starting to analyze recordings.")
        current_time = time.time()
        files = sorted(
            os.listdir(RECORDING_DIR),
            key=lambda x: os.path.getctime(os.path.join(RECORDING_DIR, x)),
        )
        flac_files = [f for f in files if f.endswith(".flac")]

        for filename in flac_files:
            filepath = os.path.join(RECORDING_DIR, filename)
            if current_time - os.path.getmtime(filepath) < 300:
                logging.info(f"Skipping recent file: {filepath}")
                continue

            logging.info(f"Processing file: {filepath}")
            recording_timestamp = get_creation_time(filepath)
            recording = Recording(
                analyzer,
                filepath,
                lat=LATITUDE,
                lon=LONGITUDE,
                overlap=OVERLAP,
                min_conf=MIN_CONF,
                sensitivity=SENSITIVITY,
                date=recording_timestamp,
                return_all_detections=True,
            )
            try:
                recording.analyze()
                new_source_recording_path = on_analyze_complete(
                    recording, recording_timestamp
                )
                extract_detected_segments(
                    recording, recording_timestamp, new_source_recording_path
                )
            except Exception as e:
                on_error(recording, e)  # Handle and delete corrupt files

        time.sleep(300)  # Check every 5 minutes for new files


def main():
    def signal_handler(sig, frame):
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)

    logging.info("Starting main loop.")
    while True:
        analyze_recordings()
        time.sleep(300)  # Check every 5 minute


if __name__ == "__main__":
    main()
