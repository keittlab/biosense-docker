#!/usr/bin/env python3

import os
import time
import signal
import sys
from datetime import datetime
from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Index, Boolean, inspect
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import toml
import logging
from logging.handlers import RotatingFileHandler
from birdnetlib import Recording
from birdnetlib.analyzer import Analyzer
import shutil

# Configure logging
log_dir = '/var/log/biosense/'
log_file = os.path.join(log_dir, 'run-birdnet.log')
log_handler = RotatingFileHandler(log_file, maxBytes=1*1024*1024, backupCount=5)  # 1 MB per file, 5 backup files
logging.basicConfig(level=logging.INFO, handlers=[log_handler],
                    format='%(asctime)s - %(levelname)s - %(message)s')

# Function to read configuration with error handling
def read_config(config_file):
    try:
        if not os.path.exists(config_file):
            raise Exception(f"Configuration file does not exist: {config_file}")

        logging.info(f"Reading configuration file: {config_file}")
        with open(config_file, 'r') as f:
            content = toml.load(f)
        
        if not content:
            raise Exception("Configuration file is empty or not found.")
        
        return content['DEFAULT']
    except Exception as e:
        logging.error(f"Error reading configuration file: {e}")
        sys.exit(1)

# Read configuration
config_file = '/etc/biosense/run-birdnet.conf'
config = read_config(config_file)

RECORDING_DIR = config.get('RECORDING_DIR', '/home/biosense/datastore/sound_recordings/')
EXPORT_DIR = config.get('EXPORT_DIR', '/home/biosense/datastore/sound_analysis_results/')
PROCESSED_DIR = config.get('PROCESSED_DIR', '/home/biosense/datastore/processed_recordings/')
SEGMENT_LENGTH = int(config.get('SEGMENT_LENGTH', 3))
DB_PATH = config.get('DB_PATH', '/home/biosense/datastore/birdnet_results.db')  # For SQLite
MIN_CONF = float(config.get('MIN_CONF', 0.1))  # Read min_conf from config
OVERLAP = float(config.get('OVERLAP', 1.5))  # Read overlap from config
SENSITIVITY = float(config.get('SENSITIVITY', 1.0))  # Read sensitivity from config

# Get latitude and longitude from environment variables
LATITUDE = float(os.getenv('LATITUDE', '0.0'))
LONGITUDE = float(os.getenv('LONGITUDE', '0.0'))
HOSTNAME = os.uname().nodename.replace('-', '_')

# SQLAlchemy setup
Base = declarative_base()

class Detection(Base):
    __tablename__ = 'birdnet_detections'
    id = Column(Integer, primary_key=True, autoincrement=True)
    hostname = Column(String)
    latitude = Column(Float)
    longitude = Column(Float)
    timestamp = Column(DateTime)  # Timestamp of the recording
    start_time = Column(Float)
    end_time = Column(Float)
    sci_name = Column(String, nullable=False)
    com_name = Column(String, nullable=False)
    confidence = Column(Float)
    is_likely = Column(Boolean, nullable=True) 
    source_recording = Column(String, nullable=True)
    file_path = Column(String, nullable=True)

Index('idx_sci_name', Detection.sci_name)

def create_session(engine):
    Session = sessionmaker(bind=engine)
    return Session()

def validate_database(engine, expected_columns):
    try:
        inspector = inspect(engine)
        if 'birdnet_detections' in inspector.get_table_names():
            columns = inspector.get_columns('birdnet_detections')
            column_names = [column['name'] for column in columns]
            if column_names == expected_columns:
                logging.info("Database schema is valid.")
                return True
            else:
                logging.warning(f"Database schema mismatch. Expected columns: {expected_columns}, Found columns: {column_names}")
        else:
            logging.warning("Table 'birdnet_detections' does not exist.")
        return False
    except Exception as e:
        logging.error(f"Error inspecting database schema: {e}")
        return False

def initialize_database(db_path, expected_columns):
    engine = create_engine(f'sqlite:///{db_path}')
    if os.path.exists(db_path):
        if not validate_database(engine, expected_columns):
            backup_db_path = f"{db_path}.backup_{datetime.now().strftime('%Y%m%d%H%M%S')}"
            logging.warning(f"Database schema does not match. Moving existing database to {backup_db_path}")
            shutil.move(db_path, backup_db_path)
            engine = create_engine(f'sqlite:///{db_path}')
    Base.metadata.create_all(engine)
    return create_session(engine)

# Expected columns in the database, maintaining order
expected_columns = ['id', 'hostname', 'latitude', 'longitude', 'timestamp', 'start_time', 'end_time', 'sci_name', 'com_name', 'confidence', 'is_likely', 'source_recording', 'file_path']

# Initialize database
session = initialize_database(DB_PATH, expected_columns)

def get_modification_time(filepath):
    try:
        modification_time = os.path.getmtime(filepath)
        return datetime.fromtimestamp(modification_time)
    except Exception as e:
        logging.error(f"Error getting modification time for file {filepath}: {e}")
        return None

def analyze_recordings():
    try:
        analyzer = Analyzer()
    except Exception as e:
        logging.error(f"Failed to initialize Analyzer: {e}")
        return

    logging.info("Starting to analyze recordings.")
    files = sorted(os.listdir(RECORDING_DIR), key=lambda x: os.path.getmtime(os.path.join(RECORDING_DIR, x)))
    flac_files = [f for f in files if f.endswith(".flac")]

    current_time = time.time()

    for filename in flac_files:
        filepath = os.path.join(RECORDING_DIR, filename)
        # Skip files modified in the last 900 seconds (15 minutes)
        if current_time - os.path.getmtime(filepath) < 900:
            logging.info(f"Skipping recent file: {filepath}")
            continue

        logging.info(f"Processing file: {filepath}")
        recording_timestamp = get_modification_time(filepath)
        recording = Recording(
            analyzer, 
            filepath,
            lat=LATITUDE, 
            lon=LONGITUDE, 
            overlap=OVERLAP,
            min_conf=MIN_CONF,
            sensitivity=SENSITIVITY,
            date=recording_timestamp,
            return_all_detections=True
        )
        try:
            recording.analyze()
            new_source_recording_path = on_analyze_complete(recording, recording_timestamp)
            extract_detected_segments(recording, recording_timestamp, new_source_recording_path)
        except Exception as e:
            on_error(recording, e)  # Handle and delete corrupt files

def main():
    def signal_handler(sig, frame):
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)

    logging.info("Starting main loop.")
    while True:
        analyze_recordings()
        time.sleep(300)  # Check every 5 minutes

if __name__ == "__main__":
    main()
