#!/usr/bin/env python3

import os
import sys
import time
import json
import signal
import shutil
import re
from datetime import datetime, timedelta
import logging
from logging.handlers import RotatingFileHandler
import numpy as np

from sqlalchemy import (
    create_engine,
    Column,
    Integer,
    String,
    Float,
    DateTime,
    Index,
    Boolean,
    inspect,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

import toml
from birdnetlib import Recording
from birdnetlib.analyzer import Analyzer


# Function to read configuration with error handling
def read_config(config_file):
    try:
        if not os.path.exists(config_file):
            raise Exception(f"Configuration file does not exist: {config_file}")

        logging.info(f"Reading configuration file: {config_file}")
        with open(config_file, "r") as f:
            content = toml.load(f)

        if not content:
            raise Exception("Configuration file is empty or not found.")

        return content["DEFAULT"]
    except Exception as e:
        logging.error(f"Error reading configuration file: {e}")
        sys.exit(1)


# Read configuration
config_file = "/etc/biosense/run-birdnet.conf"
config = read_config(config_file)

# Configure logging based on DEBUG setting
log_dir = "/var/log/biosense/"
log_file = os.path.join(log_dir, "run-birdnet.log")
log_handler = RotatingFileHandler(log_file, maxBytes=1 * 1024 * 1024, backupCount=5)

debug_mode = config.get("DEBUG", "false").lower() == "true"
logging.basicConfig(
    level=logging.DEBUG if debug_mode else logging.INFO,
    handlers=[log_handler],
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logging.info(f"Debug mode is {'enabled' if debug_mode else 'disabled'}")

# Extract configuration values
RECORDING_DIR = config.get(
    "RECORDING_DIR", "/home/biosense/datastore/sound_recordings/"
)
EXPORT_DIR = config.get(
    "EXPORT_DIR", "/home/biosense/datastore/sound_analysis_results/"
)
PROCESSED_DIR = config.get(
    "PROCESSED_DIR", "/home/biosense/datastore/processed_recordings/"
)
SEGMENT_LENGTH = int(config.get("SEGMENT_LENGTH", 3))
DB_PATH = config.get("DB_PATH", "/home/biosense/datastore/birdnet_results.db")
MIN_CONF = float(config.get("MIN_CONF", 0.1))
OVERLAP = float(config.get("OVERLAP", 1.5))
SENSITIVITY = float(config.get("SENSITIVITY", 1.0))

# Handle empty or invalid latitude/longitude values
try:
    LATITUDE = float(os.getenv("LATITUDE", "0.0"))
    LONGITUDE = float(os.getenv("LONGITUDE", "0.0"))
except ValueError:
    LATITUDE, LONGITUDE = 0.0, 0.0
    logging.warning(
        "Invalid LATITUDE or LONGITUDE environment variable; defaulting to 0.0 for both."
    )

HOSTNAME = os.uname().nodename.replace("-", "_")

# SQLAlchemy setup
Base = declarative_base()


class Detection(Base):
    __tablename__ = "birdnet_detections"
    id = Column(Integer, primary_key=True, autoincrement=True)
    hostname = Column(String)
    latitude = Column(Float)
    longitude = Column(Float)
    timestamp = Column(DateTime)
    start_time = Column(Float)
    end_time = Column(Float)
    sci_name = Column(String, nullable=False)
    com_name = Column(String, nullable=False)
    confidence = Column(Float)
    is_likely = Column(Boolean, nullable=True)
    source_recording = Column(String, nullable=True)
    file_path = Column(String, nullable=True)
    embedding = Column(String, nullable=True)

    __table_args__ = (
        Index("idx_com_name_timestamp", "com_name", "timestamp"),
        Index("idx_start_end_time", "start_time", "end_time"),
        Index("idx_com_name_confidence", "com_name", "confidence"),
        Index("idx_sci_name", "sci_name"),
    )


# Create the engine and session
engine = create_engine(f"sqlite:///{DB_PATH}")
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)
session = Session()


def validate_database(engine, expected_columns):
    try:
        inspector = inspect(engine)
        if "birdnet_detections" in inspector.get_table_names():
            columns = inspector.get_columns("birdnet_detections")
            column_names = [column["name"] for column in columns]
            if column_names == expected_columns:
                logging.info("Database schema is valid.")
                return True
            else:
                logging.warning(
                    f"Database schema mismatch. Expected columns: {expected_columns}, Found columns: {column_names}"
                )
        else:
            logging.warning("Table 'birdnet_detections' does not exist.")
        return False
    except Exception as e:
        logging.error(f"Error inspecting database schema: {e}")
        return False


# Additional debug logging within key functions
def get_creation_time(filepath):
    try:
        match = re.search(r"_(\d{8}_\d{6})-(\d+)", filepath)
        if match:
            timestamp_str = match.group(1)
            segment_number = int(match.group(2))
            initial_time = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
            segment_offset = timedelta(minutes=15 * (segment_number - 1))
            logging.debug(
                f"Parsed timestamp for {filepath}: {initial_time + segment_offset}"
            )
            return initial_time + segment_offset
        else:
            logging.error(f"No valid timestamp found in filename: {filepath}")
            return None
    except Exception as e:
        logging.error(f"Error parsing timestamp from filename {filepath}: {e}")
        return None


def create_partitioned_dir(base_dir, timestamp):
    year = timestamp.strftime("%Y")
    month = timestamp.strftime("%m")
    day = timestamp.strftime("%d")
    partitioned_dir = os.path.join(base_dir, year, month, day)
    os.makedirs(partitioned_dir, exist_ok=True)
    return partitioned_dir


def on_analyze_complete(recording, recording_timestamp):
    logging.info(f"Analyzed: {recording.path}")
    logging.debug(f"Detections: {recording.detections}")

    # Check for any human vocal detections
    if any(
        detection.get("common_name") == "Human vocal"
        for detection in recording.detections
    ):
        if os.path.exists(recording.path):
            try:
                logging.debug(
                    f"Deleting file due to human vocal detection: {recording.path}"
                )
                os.remove(recording.path)
                logging.info(f"Deleted source recording file: {recording.path}")
                return None  # Indicate that the file was deleted
            except Exception as e:
                logging.error(
                    f"Error deleting source recording file {recording.path}: {e}"
                )
                return recording.path  # Return the path if deletion failed
        else:
            logging.warning(
                f"File not found when attempting to delete: {recording.path}"
            )
            return recording.path

    # Move the recording to the processed directory after successful analysis
    try:
        processed_dir_partitioned = create_partitioned_dir(
            PROCESSED_DIR, recording_timestamp
        )
        destination = os.path.join(
            processed_dir_partitioned, os.path.basename(recording.path)
        )
        if os.path.exists(recording.path):
            logging.debug(f"Moving file to processed directory: {destination}")
            shutil.move(recording.path, destination)
            logging.info(f"Moved recording to processed directory: {destination}")
            return destination
        else:
            logging.warning(f"File not found when attempting to move: {recording.path}")
            return recording.path
    except Exception as e:
        logging.error(f"Error moving recording: {recording.path} - {e}")
        return recording.path


def on_error(recording, error):
    logging.error(f"Error: {error} in {recording.path}")
    # Attempt to delete the corrupted .flac file
    if os.path.exists(recording.path):
        try:
            logging.debug(f"Attempting to delete corrupt file: {recording.path}")
            os.remove(recording.path)
            logging.info(f"Deleted corrupt .flac file: {recording.path}")
        except Exception as e:
            logging.error(f"Failed to delete corrupt .flac file {recording.path}: {e}")
    else:
        logging.warning(
            f"File not found when attempting to delete corrupt file: {recording.path}"
        )


def extract_detected_segments(recording, recording_timestamp, source_recording_path):
    export_dir_partitioned = create_partitioned_dir(EXPORT_DIR, recording_timestamp)
    if not os.path.exists(export_dir_partitioned):
        os.makedirs(export_dir_partitioned)

    recording.extract_detections_as_audio(directory=export_dir_partitioned)
    recording.extract_embeddings()  # Populate `recording.embeddings`

    for detection in recording.detections:
        old_segment_filename = detection["extracted_audio_path"]
        new_segment_filename = os.path.join(
            export_dir_partitioned, os.path.basename(old_segment_filename)
        )

        # Handle file deletion for human vocal detection
        if not old_segment_filename or not os.path.exists(old_segment_filename):
            new_segment_filename = None
        else:
            if detection.get("common_name") == "Human vocal":
                try:
                    os.remove(old_segment_filename)
                    new_segment_filename = None
                except Exception as e:
                    logging.error(
                        f"Error deleting human vocal file {old_segment_filename}: {e}"
                    )
            else:
                shutil.move(old_segment_filename, new_segment_filename)

        embedding = next(
            (
                emb["embeddings"]
                for emb in recording.embeddings
                if emb["start_time"] == detection["start_time"]
            ),
            None,
        )
        embedding_json = json.dumps(embedding) if embedding else None

        detection_data = Detection(
            hostname=HOSTNAME,
            latitude=LATITUDE,
            longitude=LONGITUDE,
            timestamp=recording_timestamp,
            start_time=detection["start_time"],
            end_time=detection["end_time"],
            sci_name=detection["scientific_name"],
            com_name=detection["common_name"],
            confidence=detection["confidence"],
            is_likely=detection["is_predicted_for_location_and_date"],
            source_recording=source_recording_path,
            file_path=new_segment_filename,
            embedding=embedding_json,
        )

        try:
            session.add(detection_data)
            session.commit()
            logging.debug("Detection with embedding written to database successfully.")
        except Exception as e:
            logging.error(f"Error writing to database: {e}")
            session.rollback()


def analyze_recordings():
    try:
        analyzer = Analyzer()
    except Exception as e:
        logging.error(f"Failed to initialize Analyzer: {e}")
        return

    while True:
        logging.info("Starting analysis of recordings.")
        files = sorted(
            os.listdir(RECORDING_DIR),
            key=lambda x: os.path.getctime(os.path.join(RECORDING_DIR, x)),
        )
        flac_files = [f for f in files if f.endswith(".flac")]

        for filename in flac_files:
            filepath = os.path.join(RECORDING_DIR, filename)
            if time.time() - os.path.getmtime(filepath) < 900:
                logging.debug(f"Skipping recent file: {filepath}")
                continue

            logging.info(f"Processing file: {filepath}")
            recording_timestamp = get_creation_time(filepath)
            recording = Recording(
                analyzer,
                filepath,
                lat=LATITUDE,
                lon=LONGITUDE,
                overlap=OVERLAP,
                min_conf=MIN_CONF,
                sensitivity=SENSITIVITY,
                date=recording_timestamp,
                return_all_detections=True,
            )
            try:
                recording.analyze()
                new_source_recording_path = on_analyze_complete(
                    recording, recording_timestamp
                )
                extract_detected_segments(
                    recording, recording_timestamp, new_source_recording_path
                )
            except Exception as e:
                logging.error(f"Error analyzing file {filepath}: {e}")
                on_error(recording, e)
        time.sleep(300)


def main():
    signal.signal(signal.SIGINT, lambda sig, frame: sys.exit(0))
    logging.info("Starting main loop.")
    analyze_recordings()


if __name__ == "__main__":
    main()
